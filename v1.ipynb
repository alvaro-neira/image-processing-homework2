{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tarea2_object_detection_yolo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IyNy2c94qU9"
      },
      "source": [
        "# Clase 6: detección de objetos\n",
        "\n",
        "**Profesor de Cátedra:** José M. Saavedra.\n",
        "\n",
        "**Profesor Auxiliar:** Cristóbal Loyola (cloyola@dcc.uchile.cl)\n",
        "\n",
        "En la clase práctica de hoy, resolveremos un problema de detección de objetos con dos modelos distintos: YoloV5 y RetinaNet. Específicamente, estaremos interesados en detectar dígitos manuscritos (10 clases distintas) en imágenes de cheques. Los datos se pueden descargar desde el siguiente link: https://www.dropbox.com/s/yf6uhsez5f2rau0/orand-car-with-bbs.zip?dl=0 (la contraseña es *cc7221*). Como el link necesita una contraseña, los datos se descargarán desde Google Drive y no directamente usando wget.\n",
        "\n",
        "Para montar el contenido de Google Drive en la sesión de Colab, pinchamos el botón de \"Activar unidad de Drive\" en el menú de la izquierda. Luego podemos usar el comando *cp* para copiar los datos desde Drive a Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSF8Cr4o0RBs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04686a91-c620-4ac9-a3da-0a376c9f7488"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laYOFrjTC_90",
        "outputId": "68aedb88-66d1-4ba0-abcd-854b32d55871"
      },
      "source": [
        "# Copiamos los datos al entorno de Colab\n",
        "!mkdir /content/data\n",
        "%cd /content/data\n",
        "!cp /content/drive/MyDrive/t2-data/orand-car-with-bbs.zip .\n",
        "!unzip -q orand-car-with-bbs.zip\n",
        "!rm orand-car-with-bbs.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/data\n",
            "cp: cannot stat '/content/drive/MyDrive/t2-data/orand-car-with-bbs.zip': No such file or directory\n",
            "unzip:  cannot find or open orand-car-with-bbs.zip, orand-car-with-bbs.zip.zip or orand-car-with-bbs.zip.ZIP.\n",
            "rm: cannot remove 'orand-car-with-bbs.zip': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbQxsuaTFJPz"
      },
      "source": [
        "## YOLOv5\n",
        "\n",
        "La implementación de este modelo se puede encontrar en el siguiente repositorio: https://github.com/ultralytics/yolov5 (para más información, ir [aquí](https://towardsai.net/p/computer-vision/yolo-v5%E2%80%8A-%E2%80%8Aexplained-and-demystified)). Para llevar a cabo el entrenamiento sobre un dataset personalizado, pueden seguir este tutorial: https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data. Los pasos más relevantes son crear un archivo de configuración que contenga la ruta de las imágenes, la cantidad de clases y los nombres de las categorías. Además, debemos convertir las coordenadas de las anotaciones desde el formato [class_idx, xmin, ymin, width, height] que viene con el dataset a un formato [class_idx, x_center, y_center, width, height], donde las coordenadas están normalizadas por el tamaño de la imagen. El resto del proceso se puede llevar a cabo tal como se indica en el tutorial y en el README del repositorio. Para modificar los parámetros de detección, pueden revisar los argumentos del archivo *detect.py* (incluye la opción de NMS)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYZ1q9ADNkiT"
      },
      "source": [
        "### Convertir coordenadas\n",
        "\n",
        "El primer paso será convertir el formato de las anotaciones como se mencionó anteriormente. El código requiere tener las imágenes en un directorio llamado 'images' y las anotaciones en un directorio llamado 'labels'.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNns7DhRD0U3",
        "outputId": "cd0796a4-6a03-4edc-bce5-5c4054b213f8"
      },
      "source": [
        "%cd /content/data/orand-car-with-bbs/training\n",
        "!mkdir labels\n",
        "\n",
        "import cv2\n",
        "import glob\n",
        "import os\n",
        "\n",
        "filenames = glob.glob(\"./annotations/*.txt\")\n",
        "\n",
        "# Iteramos sobre todas las imágenes\n",
        "for filename in filenames:\n",
        "\n",
        "    # Abrimos la imagen para obtener sus dimensiones\n",
        "    img_path = filename.split(\"/\")[-1].replace(\".txt\", \".png\")\n",
        "    img_path = os.path.join(\"./images\", img_path)\n",
        "    img = cv2.imread(img_path)\n",
        "    img_height, img_width, _ = img.shape\n",
        "\n",
        "    # Obtenemos las coordenadas de cada objeto en la imagen\n",
        "    with open(filename, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        lines = [x.strip() for x in lines]\n",
        "        lines = [x.replace(' ', '') for x in lines]\n",
        "\n",
        "    annotations_yolo = []\n",
        "    # Iteramos sobre todas las anotaciones de la imagen\n",
        "    for obj in lines:\n",
        "        label = obj.split(\":\")[0]\n",
        "        bbox = obj.split(\":\")[1]\n",
        "        bbox = bbox.replace(\",\", \" \")\n",
        "\n",
        "        annotation = label + \" \" + bbox\n",
        "        ann = annotation.split(\" \")\n",
        "        ann = [int(x) for x in ann]\n",
        "        label = ann[0]\n",
        "        xmin, ymin, width, height = ann[1:]\n",
        "\n",
        "        # Obtenemos las coordenadas normalizadas\n",
        "        x_center = (xmin + (width / 2)) / img_width\n",
        "        y_center = (ymin + (height/ 2)) / img_height\n",
        "        width = width / img_width\n",
        "        height = height / img_height\n",
        "\n",
        "        obj_info = [label, x_center, y_center, width, height]\n",
        "        annotations_yolo.append(obj_info)\n",
        "\n",
        "    # Escribimos las nuevas anotaciones en un archivo del mismo nombre pero \n",
        "    # en el directorio labels\n",
        "    new_filename = filename.split(\"/\")[-1]\n",
        "    new_filename = os.path.join(\"./labels\", new_filename)\n",
        "    with open(new_filename, mode='w') as outfile:\n",
        "        for annotation in annotations_yolo:\n",
        "            annotation = [str(x) for x in annotation]\n",
        "            line = ' '.join(annotation)\n",
        "            outfile.write(\"%s\\n\" % line)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/data/orand-car-with-bbs/training'\n",
            "/content/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTZHesUWIIZf",
        "outputId": "37a07ce6-b8cf-4643-bcab-3411d27917e8"
      },
      "source": [
        "# Ejemplo de anotación antigua\n",
        "!cat /content/data/orand-car-with-bbs/training/annotations/0001_5179655_46066.txt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat: /content/data/orand-car-with-bbs/training/annotations/0001_5179655_46066.txt: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9HTiaUzMW39",
        "outputId": "d6961e1b-c3b0-4e31-d5a4-e1b7fbeaf5ae"
      },
      "source": [
        "# Ejemplo de la misma anotación pero con el nuevo formato\n",
        "!cat /content/data/orand-car-with-bbs/training/labels/0001_5179655_46066.txt"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat: /content/data/orand-car-with-bbs/training/labels/0001_5179655_46066.txt: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pWRH-3dOiD0"
      },
      "source": [
        "Ahora dividiremos los datos en un conjunto de entrenamiento y uno de test, en una proporción 90%-10% respectivamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVKCQ4u1PTaS",
        "outputId": "5508b9fd-25da-435f-d863-303bff67cf52"
      },
      "source": [
        "%cd /content/data/orand-car-with-bbs/training\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "filenames = glob.glob(\"images/*.png\")\n",
        "random.shuffle(filenames)\n",
        "\n",
        "n = len(filenames)\n",
        "split_ratio = 0.9\n",
        "split_index = np.rint(n * split_ratio).astype(\"int\")\n",
        "\n",
        "train_split = filenames[:split_index]\n",
        "test_split = filenames[split_index:]\n",
        "\n",
        "train_dir = \"/content/data/orand-car-with-bbs/training/\"\n",
        "\n",
        "with open(\"train.txt\", mode='w') as outfile:\n",
        "    for annotation in train_split:\n",
        "        annotation = os.path.join(train_dir, annotation)\n",
        "        outfile.write(\"%s\\n\" % annotation)\n",
        "\n",
        "with open(\"test.txt\", mode='w') as outfile:\n",
        "    for annotation in test_split:\n",
        "        annotation = os.path.join(train_dir, annotation)\n",
        "        outfile.write(\"%s\\n\" % annotation)\n",
        "\n",
        "print(f\"Total de imágenes: {n}\")\n",
        "print(f\"Imágenes de entrenamiento: {len(train_split)}\")\n",
        "print(f\"Imágenes de test: {len(test_split)}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/data/orand-car-with-bbs/training'\n",
            "/content/data\n",
            "Total de imágenes: 0\n",
            "Imágenes de entrenamiento: 0\n",
            "Imágenes de test: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8X1cllIM315"
      },
      "source": [
        "### Entrenamiento\n",
        "\n",
        "Para el entrenamiento seguiremos los siguientes pasos:\n",
        "\n",
        "* Clonar el repositorio de YOLOv5.\n",
        "* Instalar las librerías requeridas.\n",
        "* Modificar los archivos de configuración necesarios."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHafx4BaMsL-",
        "outputId": "c2dd994e-5081-4ba8-cb31-ee521bf306bd"
      },
      "source": [
        "# Clonamos el repositorio\n",
        "%cd /content\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "\n",
        "# Instalamos los requisitos\n",
        "%cd /content/yolov5\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 10008, done.\u001b[K\n",
            "remote: Total 10008 (delta 0), reused 0 (delta 0), pack-reused 10008\u001b[K\n",
            "Receiving objects: 100% (10008/10008), 10.35 MiB | 31.00 MiB/s, done.\n",
            "Resolving deltas: 100% (6938/6938), done.\n",
            "/content/yolov5\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (1.19.5)\n",
            "Requirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (4.1.2.30)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (7.1.2)\n",
            "Collecting PyYAML>=5.3.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (2.23.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (1.4.1)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (0.11.1+cu111)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (4.62.3)\n",
            "Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 16)) (2.7.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 20)) (1.1.5)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 21)) (0.11.2)\n",
            "Collecting thop\n",
            "  Downloading thop-0.0.31.post2005241907-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (3.0.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->-r requirements.txt (line 11)) (3.10.0.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.6.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (57.4.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (3.17.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.37.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.35.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.42.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.12.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (3.3.6)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 20)) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 16)) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 16)) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 16)) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 16)) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 16)) (3.1.1)\n",
            "Installing collected packages: thop, PyYAML\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-6.0 thop-0.0.31.post2005241907\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7CP7xW1RjNX"
      },
      "source": [
        "Ahora agregaremos el siguiente archivo de configuración en el directorio ```/content/yolov5/data```, con el nombre *digits.yaml*:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "train: /content/data/orand-car-with-bbs/training/train.txt\n",
        "val: /content/data/orand-car-with-bbs/training/test.txt\n",
        "\n",
        "# number of classes\n",
        "nc: 10\n",
        "\n",
        "# class names\n",
        "names: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plJy8lWEOO2_"
      },
      "source": [
        "# Entrenamos el modelo\n",
        "# %cd /content/yolov5\n",
        "# !python train.py --img 640 --batch 16 --epochs 10 --data digits.yaml --weights yolov5s.pt"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17Ze2nNGToja"
      },
      "source": [
        "# Guarda resultados en drive\n",
        "# !cp -r /content/yolov5/runs/train /content/drive/MyDrive/t2-train"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPivHRhLwwwj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5caf9928-ba75-4051-d194-6da0abba5800"
      },
      "source": [
        "# Restaura resultados en yolo\n",
        "!mkdir /content/yolov5/runs/\n",
        "!cp -r /content/drive/MyDrive/t2-train/train /content/yolov5/runs/"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/drive/MyDrive/t2-train/train': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QszgMBPkXaRy"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T74ltdgkVSdS"
      },
      "source": [
        "# %cd /content/yolov5\n",
        "# !python detect.py --source /content/data/orand-car-with-bbs/test/images --weights runs/train/exp/weights/best.pt --conf 0.25 --save-txt"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogMjQrChevar"
      },
      "source": [
        "# !cp -r /content/yolov5/runs/detect /content/drive/MyDrive/t2-detect"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZ5cGtIIDPJT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6baaa60-fd93-416a-ba27-6eedc556e6ab"
      },
      "source": [
        "# Restaurar datos detectados\n",
        "!cp -r  /content/drive/MyDrive/t2-detect/detect/ /content/yolov5/runs/"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/drive/MyDrive/t2-detect/detect/': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "id": "yRb4MdPenmU-",
        "outputId": "b1e9f74b-4e0f-4da3-b281-06f66135ce88"
      },
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image('/content/yolov5/runs/train/exp/confusion_matrix.png'))\n",
        "display(Image('/content/yolov5/runs/train/exp/results.png'))\n",
        "display(Image('/content/yolov5/runs/train/exp/val_batch2_labels.jpg'))\n",
        "display(Image('/content/yolov5/runs/train/exp/val_batch2_pred.jpg'))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "/content/yolov5/runs/train/exp/confusion_matrix.png",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "/content/yolov5/runs/train/exp/results.png",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "/content/yolov5/runs/train/exp/val_batch2_labels.jpg",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "/content/yolov5/runs/train/exp/val_batch2_pred.jpg",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIQeHDkgIXMH"
      },
      "source": [
        "# Cálculo de accurracy conjunto de test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HE6rtxNgK0Yt"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import glob"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDOkIg3MIgXi"
      },
      "source": [
        "## Pruebas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "id": "3aKxDsua3Qgy",
        "outputId": "d36e5b0f-b415-477a-d82f-6a8391cfcc58"
      },
      "source": [
        "img = cv2.imread(\"/content/yolov5/runs/detect/exp/handwritten_MX8900201925093637024_maEnO1gFYEWnCyVkm4IPuQ.bmp\")\n",
        "plt.imshow(img)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-b73a68f50565>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/yolov5/runs/detect/exp/handwritten_MX8900201925093637024_maEnO1gFYEWnCyVkm4IPuQ.bmp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2649\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         resample=resample, url=url, **({\"data\": data} if data is not\n\u001b[0;32m-> 2651\u001b[0;31m         None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2652\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1563\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5624\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5626\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5627\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    692\u001b[0m                 not np.can_cast(self._A.dtype, float, \"same_kind\")):\n\u001b[1;32m    693\u001b[0m             raise TypeError(\"Image data of dtype {} cannot be converted to \"\n\u001b[0;32m--> 694\u001b[0;31m                             \"float\".format(self._A.dtype))\n\u001b[0m\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m         if not (self._A.ndim == 2\n",
            "\u001b[0;31mTypeError\u001b[0m: Image data of dtype object cannot be converted to float"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCSgUyUB3U_J"
      },
      "source": [
        "!cat /content/yolov5/runs/detect/exp/labels/handwritten_MX8900201925093637024_maEnO1gFYEWnCyVkm4IPuQ.txt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70ljnvYs5Q8A"
      },
      "source": [
        "#ordenando los objetos encontrados de izquierda a derecha (por ejemplo, \n",
        "#en base a la coordenada x de la esquina superior izquierda del bounding box, o a la coordenada x del centro).\n",
        "data = pd.read_csv('/content/yolov5/runs/detect/exp/labels/handwritten_MX8900201925093637024_maEnO1gFYEWnCyVkm4IPuQ.txt', header = None, sep=\" \")\n",
        "data = data.sort_values(by=1)\n",
        "values = data[0].values.tolist()\n",
        "int(''.join(str(val) for val in values))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHWmW5nOIlZ5"
      },
      "source": [
        "## Cálculo total"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyzJx_56LITo"
      },
      "source": [
        "!cp /content/data/orand-car-with-bbs/test/list.txt /content/yolov5/runs/list.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJXjGvsl6lMW"
      },
      "source": [
        "class Metrics:\n",
        "  def __init__(self):\n",
        "    self.positives = []\n",
        "    self.negatives = []\n",
        "    self.accuraccy = 0\n",
        "\n",
        "  def calculate_accuracy(self):\n",
        "\n",
        "      # Definición de los archivos de detección a recorrer\n",
        "      data_dir = \"/content/yolov5/runs/detect/exp/labels/\"\n",
        "      labels_files = glob.glob(data_dir + \"*.txt\")\n",
        "\n",
        "      # Las etiquetas reales se llevan a DataFrame\n",
        "      real = pd.read_csv('/content/yolov5/runs/list.txt', header = None, sep=\"\\t\")\n",
        "\n",
        "      # Contadores para cálculo de métricas\n",
        "      n_positive = 0\n",
        "      n_negative = 0\n",
        "      n_total = len(labels_files)\n",
        "\n",
        "      # Iteramos sobre los archivos de detección\n",
        "      for idx, label_file in enumerate(labels_files):\n",
        "\n",
        "        # Se obtiene el nombre del archivo txt y su imagen correspondiente\n",
        "        name_file = label_file.split('/')[7]\n",
        "        img_file = name_file.replace('.txt', '.bmp')\n",
        "\n",
        "        # Se obtiene el valor detectado en el archivo utilizando DataFrame\n",
        "        # ordenando los dígitos encontrados en base a la coordenada x\n",
        "        data = pd.read_csv(data_dir+name_file, header = None, sep=\" \")\n",
        "        data = data.sort_values(by=1)\n",
        "\n",
        "        # Se concatenan los dígitos para obtener el valor final\n",
        "        values = data[0].values.tolist()\n",
        "        amount = int(''.join(str(val) for val in values))\n",
        "\n",
        "        # Se obtiene el valor real del archivo\n",
        "        real_amount = (real.loc[real[0] == img_file])[1].values[0]\n",
        "\n",
        "        record = {\n",
        "            'file': img_file,\n",
        "            'real': real_amount,\n",
        "            'detected': amount,\n",
        "        }\n",
        "        \n",
        "        # Se compara el monto detectado con el monto real\n",
        "        if (amount == real_amount ):\n",
        "          self.positives.append(record)\n",
        "          n_positive += 1\n",
        "        else:\n",
        "          self.negatives.append(record)\n",
        "          n_negative += 1\n",
        "\n",
        "        # Se imprime información\n",
        "        log = \"\"\"\n",
        "          ---- Calculating File {} ----\n",
        "          detected amount: {}\n",
        "          real amount:     {}\n",
        "          positive:        {}\n",
        "          -----------------------------\n",
        "          \"\"\".format(\n",
        "              name_file,\n",
        "              amount,\n",
        "              real_amount,\n",
        "              amount == real_amount\n",
        "          )\n",
        "        print(log)\n",
        "\n",
        "      # Se calcula el accuraccy\n",
        "      self.accuraccy = n_positive / n_total\n",
        "\n",
        "      # Muestra resultados\n",
        "      result = {\n",
        "          'n_positive: ': n_positive,\n",
        "          'n_negative: ': n_negative,\n",
        "          'n_total: ': n_total,\n",
        "          'accuracy': self.accuraccy\n",
        "      }\n",
        "      print(result)\n",
        "\n",
        "  def show_negative_results(self):\n",
        "    data_test_dir = \"/content/data/orand-car-with-bbs/test/images/\"\n",
        "    fig = plt.figure(figsize=(20, 40))\n",
        "    rows = 15\n",
        "    columns = 5\n",
        "    position = 1\n",
        "\n",
        "    for img in self.negatives:\n",
        "      image = cv2.imread(data_test_dir+img['file'])\n",
        "      title = \"real: {} \\n detected: {}\".format(img['real'], img['detected'])\n",
        "      fig.add_subplot(rows, columns, position)\n",
        "      plt.imshow(image)\n",
        "      plt.axis('off')\n",
        "      plt.title(title)\n",
        "      position += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h92JVMp57efr"
      },
      "source": [
        "m = Metrics()\n",
        "m.calculate_accuracy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJnd77lwMdAx"
      },
      "source": [
        "**Se obtiene un 83,8% de accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iJc_0ehPR51"
      },
      "source": [
        "m.show_negative_results()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGIym346MW-S"
      },
      "source": [
        "real = pd.read_csv('/content/yolov5/runs/list.txt', header = None, sep=\"\\t\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eu3SWSS1tPzH"
      },
      "source": [
        "(real.loc[real[1] == 317492]).values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjFZYptkvHxE"
      },
      "source": [
        "(real.loc[real[1] == 2832199]).values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOWf_Yd2vVZZ"
      },
      "source": [
        "!cat /content/yolov5/runs/detect/exp/labels/handwritten_MX8900201937105320243_UBtSmf6D9Eqar5uw_Jn_Cw.txt"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}